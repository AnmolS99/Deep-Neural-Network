; Network with five hidden layers, all activation functions are used in one or more layers
[globals]
loss: cross_entropy
include_softmax: true
num_classes: 4
regularizer: l2
reg_rate: 0.001
epochs: 50
batch_size: 7

[data_generator]
image_dimension: 20
dataset_size: 300
l_lower_frac: 0.40
l_higher_frac: 0.40
width_lower_frac: 0.02
width_higher_frac: 0.04
centering: False
noise_percentage: 0.01
train_frac: 0.70
valid_frac: 0.20
test_frac: 0.10

[layer_1]
neurons: 20
activation_function: relu
wr_lower: -0.5
wr_higher:  0.5
lr: 0.1

[layer_2]
neurons: 20
activation_function: linear
wr_lower: -0.5
wr_higher: 0.5
lr: 0.1

[layer_3]
neurons: 20
activation_function: tanh
wr_lower: -0.5
wr_higher: 0.5
lr: 0.1

[layer_4]
neurons: 250
activation_function: linear
wr_lower: -0.5
wr_higher: 0.5
lr: 0.1

[layer_5]
neurons: 20
activation_function: relu
wr_lower: -0.5
wr_higher: 0.5
lr: 0.1

[layer_6]
neurons: 4
activation_function: sigmoid
wr_lower: -0.5
wr_higher: 0.5
lr: 0.1